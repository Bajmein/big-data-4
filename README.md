# ğŸ“‰ OptimizaciÃ³n de Devoluciones en Retail (Big Data Proyecto 4)

Este proyecto implementa un pipeline de Ciencia de Datos para analizar, predecir y optimizar las devoluciones en un entorno de retail. Utiliza tÃ©cnicas de Machine Learning (ClasificaciÃ³n y Clustering) para identificar patrones de devoluciÃ³n y segmentar clientes, proporcionando una estrategia de negocio basada en datos.

## ğŸš€ CaracterÃ­sticas Principales

*   **Ingesta y Procesamiento:** FusiÃ³n de datos transaccionales y de estado (ETL).
*   **AnÃ¡lisis Exploratorio (EDA):** DetecciÃ³n de causas raÃ­z por gerente, transporte y temporalidad.
*   **Modelado Predictivo:** Clasificador **Random Forest** para predecir la probabilidad de devoluciÃ³n.
*   **SegmentaciÃ³n de Clientes:** Clustering **K-Means** para identificar perfiles de comportamiento.
*   **Reporte Automatizado:** GeneraciÃ³n de dashboards ejecutivos y reportes estratÃ©gicos.

## ğŸ› ï¸ Requisitos Previos

Antes de comenzar, asegÃºrate de tener instaladas las siguientes herramientas:

1.  **Python 3.11 o superior**
2.  **uv (Gestor de paquetes):**
    ```bash
    # Linux / macOS
    curl -LsSf https://astral.sh/uv/install.sh | sh

    # Windows
    powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
    ```
3.  **Task (Gestor de tareas, opcional pero recomendado):**
    ```bash
    # InstalaciÃ³n via npm
    npm install -g @go-task/cli

    # O visita https://taskfile.dev/installation/ para otros mÃ©todos
    ```

## ğŸ“¦ InstalaciÃ³n del Proyecto

1.  **Clonar el repositorio:**
    ```bash
    git clone <url-del-repo>
    cd bigdata-4
    ```

2.  **Instalar dependencias y entorno virtual con `uv`:**
    ```bash
    uv sync
    ```
    Este comando leerÃ¡ `pyproject.toml`, crearÃ¡ un entorno virtual en `.venv` e instalarÃ¡ todas las librerÃ­as necesarias (pandas, scikit-learn, matplotlib, etc.).

## â–¶ï¸ EjecuciÃ³n

El proyecto utiliza `Taskfile` para simplificar los comandos comunes.

### 1. Ejecutar el Pipeline Completo
Para ejecutar todo el proceso (ingesta -> modelado -> reporte):

```bash
task run
```
*Si no tienes instalado `task`, puedes usar el comando equivalente:*
```bash
uv run main.py
```

### 2. Verificar Calidad de CÃ³digo (Linting)
Para revisar y corregir el estilo del cÃ³digo:

```bash
task check
```

### ğŸ“‚ Salidas (Outputs)

Al finalizar la ejecuciÃ³n, se crearÃ¡ una carpeta `outputs/` con los siguientes artefactos:

*   `dashboard_kpis.png`: Dashboard visual con las mÃ©tricas clave (Tasa de devoluciÃ³n, F1-Score, etc.).
*   `manager_performance.png`: GrÃ¡fico de desempeÃ±o por gerente.
*   `shipping_analysis.png`: AnÃ¡lisis de devoluciones por tipo de envÃ­o.
*   `customer_segments.png`: VisualizaciÃ³n de los segmentos de clientes detectados.
*   `strategy_report.md`: Informe detallado con recomendaciones estratÃ©gicas y propuesta de arquitectura.

## ğŸ—ï¸ Estructura del Proyecto

```text
.
â”œâ”€â”€ data/                   # Archivos de entrada (Produccion.xlsx, etc.)
â”œâ”€â”€ documentacion/          # DocumentaciÃ³n (ARQUITECTURA.md, HOJA_DE_RUTA.md)
â”œâ”€â”€ outputs/                # Resultados generados (GrÃ¡ficos, Reportes)
â”œâ”€â”€ src/                    # CÃ³digo fuente
â”‚   â”œâ”€â”€ ingesta/            # Carga y validaciÃ³n de datos
â”‚   â”œâ”€â”€ procesamiento/      # TransformaciÃ³n y limpieza
â”‚   â”œâ”€â”€ analisis/           # AnÃ¡lisis exploratorio (EDA)
â”‚   â”œâ”€â”€ modelado/           # Modelos ML (Clasificador, Clustering)
â”‚   â””â”€â”€ reportes/           # GeneraciÃ³n de visualizaciones y estrategia
â”œâ”€â”€ main.py            # Punto de entrada principal
â”œâ”€â”€ notebook.ipynb          # Cuaderno de experimentaciÃ³n
â”œâ”€â”€ pyproject.toml          # ConfiguraciÃ³n de dependencias
â”œâ”€â”€ taskfile.yaml           # DefiniciÃ³n de tareas
â””â”€â”€ README.md               # Este archivo
```

## ğŸ“Š Pipeline de Datos

1.  **Ingesta:** Carga datos de `data/`, realiza un *Left Join* entre transacciones y estatus.
2.  **Procesamiento:** Imputa valores nulos, codifica variables categÃ³ricas y extrae caracterÃ­sticas temporales.
3.  **Modelado:**
    *   *ClasificaciÃ³n:* Entrena un modelo para predecir `is_returned`.
    *   *Clustering:* Agrupa clientes basado en frecuencia, monto y tasa de devoluciÃ³n.
4.  **Estrategia:** Analiza los resultados para sugerir acciones (ej. capacitaciÃ³n a gerentes especÃ­ficos) y propone una arquitectura Big Data (Kafka + Spark) para escalar la soluciÃ³n.
